{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from enum import Enum\n",
    "\n",
    "class Activation(Enum):\n",
    "    \"\"\"Enum for activation functions.\"\"\"\n",
    "\n",
    "    GELU = 1\n",
    "    RELU = 2\n",
    "class MLP(torch.nn.Module):\n",
    "\n",
    "    linear1: torch.nn.Linear\n",
    "    linear2: torch.nn.Linear\n",
    "    activation: Activation\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        size: int,\n",
    "        hidden_size: int,\n",
    "        activation: Activation | str,\n",
    "        *,\n",
    "        device: torch.device | None,\n",
    "        dtype: torch.dtype | None,\n",
    "        initialize_output_to_zero: bool = False,\n",
    "        recompute: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(\n",
    "            size,\n",
    "            hidden_size,\n",
    "            bias=False,\n",
    "            device=device,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "        self.linear2 = torch.nn.Linear(\n",
    "            hidden_size,\n",
    "            size,\n",
    "            bias=False,\n",
    "            device=device,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "        if isinstance(activation, str):\n",
    "            activation = Activation[activation.upper()]\n",
    "        self.activation = activation\n",
    "        if initialize_output_to_zero:\n",
    "            torch.nn.init.zeros_(self.linear2.weight)\n",
    "        if recompute:\n",
    "            self.forward = partial(checkpoint, self.forward, use_reentrant=False)  # type: ignore\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x0: torch.Tensor,\n",
    "        add_input: bool = False,\n",
    "        allow_inplace: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        x = self.linear1(x0)\n",
    "        if self.activation is Activation.GELU:\n",
    "            x = torch.nn.functional.gelu(x)\n",
    "        elif self.activation is Activation.RELU:\n",
    "            x = torch.nn.functional.relu(x)\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                f\"Activation Function {self.activation} is not implemented.\",\n",
    "            )\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        if add_input:\n",
    "            x += x0\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerFeatureEncoderLayer(Module):\n",
    "    __constants__: ClassVar = [\"batch_first\"]\n",
    "\n",
    "    def __init__(  # noqa: PLR0913\n",
    "        self,\n",
    "        *,\n",
    "        d_model: int,\n",
    "        nhead: int,\n",
    "        dim_feedforward: int | None = None,\n",
    "        activation: str = \"relu\",\n",
    "        layer_norm_eps: float = 1e-5,\n",
    "        pre_norm: bool = False,\n",
    "        device: torch.device | None = None,\n",
    "        dtype: torch.dtype | None = None,\n",
    "        recompute_attn: bool = False,\n",
    "        second_mlp: bool = False,\n",
    "        layer_norm_with_elementwise_affine: bool = False,\n",
    "        zero_init: bool = False,\n",
    "        save_peak_mem_factor: int | None = None,\n",
    "        attention_between_features: bool = True,\n",
    "        multiquery_item_attention: bool = False,\n",
    "        multiquery_item_attention_for_test_set: bool = False,\n",
    "        two_sets_of_queries: bool = False,\n",
    "        attention_init_gain: float = 1.0,\n",
    "        d_k: int | None = None,\n",
    "        d_v: int | None = None,\n",
    "        precomputed_kv: None | torch.Tensor | tuple[torch.Tensor, torch.Tensor] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        assert d_model % nhead == 0 or (d_k is not None and d_v is not None)\n",
    "        if multiquery_item_attention_for_test_set and multiquery_item_attention:\n",
    "            raise ValueError(\n",
    "                \"Cannot use both multiquery_item_attention_for_test_set\"\n",
    "                \"and multiquery_item_attention\",\n",
    "            )\n",
    "        if two_sets_of_queries and not multiquery_item_attention_for_test_set:\n",
    "            raise ValueError(\n",
    "                \"two_sets_of_queries requires multiquery_item_attention_for_test_set\",\n",
    "            )\n",
    "\n",
    "        if d_k is None:\n",
    "            d_k = d_model // nhead\n",
    "\n",
    "        if d_v is None:\n",
    "            d_v = d_model // nhead\n",
    "\n",
    "        self.self_attn_between_features: MultiHeadAttention | None = None\n",
    "        if attention_between_features:\n",
    "            self.self_attn_between_features = MultiHeadAttention(\n",
    "                input_size=d_model,\n",
    "                output_size=d_model,\n",
    "                d_k=d_k,\n",
    "                d_v=d_v,\n",
    "                nhead=nhead,\n",
    "                device=device,\n",
    "                dtype=dtype,\n",
    "                initialize_output_to_zero=zero_init,\n",
    "                recompute=recompute_attn,\n",
    "                init_gain=attention_init_gain,\n",
    "            )\n",
    "\n",
    "        if isinstance(precomputed_kv, tuple):\n",
    "            precomputed_k, precomputed_v = precomputed_kv\n",
    "            precomputed_kv = None\n",
    "        else:\n",
    "            precomputed_k = precomputed_v = None\n",
    "\n",
    "        self.self_attn_between_items = MultiHeadAttention(\n",
    "            input_size=d_model,\n",
    "            output_size=d_model,\n",
    "            d_k=d_k,\n",
    "            d_v=d_v,\n",
    "            nhead=nhead,\n",
    "            device=device,\n",
    "            dtype=dtype,\n",
    "            share_kv_across_n_heads=nhead if multiquery_item_attention else 1,\n",
    "            initialize_output_to_zero=zero_init,\n",
    "            recompute=recompute_attn,\n",
    "            precomputed_k=precomputed_k,\n",
    "            precomputed_v=precomputed_v,\n",
    "            precomputed_kv=precomputed_kv,\n",
    "            init_gain=attention_init_gain,\n",
    "            two_sets_of_queries=(\n",
    "                multiquery_item_attention_for_test_set and two_sets_of_queries\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if dim_feedforward is None:\n",
    "            dim_feedforward = 2 * d_model\n",
    "\n",
    "        self.mlp = MLP(\n",
    "            size=d_model,\n",
    "            hidden_size=dim_feedforward,\n",
    "            activation=activation,\n",
    "            device=device,\n",
    "            dtype=dtype,\n",
    "            initialize_output_to_zero=zero_init,\n",
    "            recompute=recompute_attn,\n",
    "        )\n",
    "\n",
    "        self.layer_norms = nn.ModuleList(\n",
    "            [\n",
    "                LayerNorm(\n",
    "                    d_model,  # type: ignore\n",
    "                    layer_norm_eps,\n",
    "                    elementwise_affine=layer_norm_with_elementwise_affine,\n",
    "                    **factory_kwargs,\n",
    "                )\n",
    "                for _ in range(4 if second_mlp else 3)\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        self.second_mlp: MLP | None = None\n",
    "        if second_mlp:\n",
    "            self.second_mlp = MLP(\n",
    "                size=d_model,\n",
    "                hidden_size=dim_feedforward,\n",
    "                activation=activation,\n",
    "                device=device,\n",
    "                dtype=dtype,\n",
    "                initialize_output_to_zero=zero_init,\n",
    "                recompute=recompute_attn,\n",
    "            )\n",
    "\n",
    "        self.pre_norm = pre_norm\n",
    "        self.recompute_attn = recompute_attn\n",
    "        self.save_peak_mem_factor = save_peak_mem_factor\n",
    "        self.multiquery_item_attention_for_test_set = (\n",
    "            multiquery_item_attention_for_test_set\n",
    "        )\n",
    "        self.two_sets_of_queries = two_sets_of_queries\n",
    "\n",
    "    def __setstate__(self, state: dict[str, Any]) -> None:\n",
    "        state.setdefault(\"save_peak_mem_factor\", None)\n",
    "        super().__setstate__(state)\n",
    "\n",
    "    def forward(  # noqa: C901\n",
    "        self,\n",
    "        state: Tensor,\n",
    "        single_eval_pos: int | None = None,\n",
    "        *,\n",
    "        cache_trainset_representation: bool = False,\n",
    "        att_src: Tensor | None = None,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            state:\n",
    "                The transformer state passed as input to the layer of shape\n",
    "                (batch_size, num_items, num_feature_blocks, d_model).\n",
    "            single_eval_pos:\n",
    "                The position from which on everything is treated as test\n",
    "                set.\n",
    "            cache_trainset_representation:\n",
    "                Whether to cache the trainset representation.\n",
    "                If single_eval_pos is set (> 0 and not None), create a cache of the\n",
    "                trainset KV. This may require a lot of memory. Otherwise, use\n",
    "                cached KV representations for inference.\n",
    "            att_src:\n",
    "                The tensor to attend to from the final layer of the encoder.\n",
    "                It has a shape of\n",
    "                (batch_size, num_train_items, num_feature_blocks, d_model).\n",
    "                This does not work with multiquery_item_attention_for_test_set and\n",
    "                cache_trainset_representation at this point.\n",
    "\n",
    "        Returns:\n",
    "            The transformer state passed through the encoder layer.\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            len(state.shape) == 4\n",
    "        ), \"src must be of shape (batch_size, num_items, num feature blocks, d_model)\"\n",
    "        if single_eval_pos is None:\n",
    "            single_eval_pos = 0\n",
    "\n",
    "        save_peak_mem_factor = self.save_peak_mem_factor\n",
    "        if cache_trainset_representation and not single_eval_pos:\n",
    "            assert self.self_attn_between_items.has_cached_kv\n",
    "            save_peak_mem_factor = None\n",
    "\n",
    "        if att_src is not None:\n",
    "            assert (\n",
    "                not self.multiquery_item_attention_for_test_set\n",
    "            ), \"Not implemented yet.\"\n",
    "            assert not cache_trainset_representation, \"Not implemented yet.\"\n",
    "            assert not single_eval_pos, (\n",
    "                \"single_eval_pos should not be set, as the train representation\"\n",
    "                \" is in att_src\"\n",
    "            )\n",
    "\n",
    "        if self.self_attn_between_features is None:\n",
    "            assert not cache_trainset_representation, \"Not implemented yet.\"\n",
    "            assert state.shape[2] == 1, (\n",
    "                f\"One group architecture expects one feature group, \"\n",
    "                f\"but got {state.shape[2]} feature groups.\"\n",
    "            )\n",
    "\n",
    "        def attn_between_features(x: torch.Tensor) -> torch.Tensor:\n",
    "            assert self.self_attn_between_features is not None\n",
    "            return self.self_attn_between_features(\n",
    "                x,\n",
    "                save_peak_mem_factor=save_peak_mem_factor,\n",
    "                add_input=True,\n",
    "                allow_inplace=True,\n",
    "            )\n",
    "\n",
    "        def attn_between_items(x: torch.Tensor) -> torch.Tensor:\n",
    "            # we need to transpose as self attention always treats\n",
    "            # dim -2 as the sequence dimension\n",
    "            if self.multiquery_item_attention_for_test_set:\n",
    "                if single_eval_pos < x.shape[1]:\n",
    "                    new_x_test = self.self_attn_between_items(\n",
    "                        x[:, single_eval_pos:].transpose(1, 2),\n",
    "                        x[:, :single_eval_pos].transpose(1, 2)\n",
    "                        if single_eval_pos\n",
    "                        else None,\n",
    "                        save_peak_mem_factor=save_peak_mem_factor,\n",
    "                        cache_kv=False,\n",
    "                        add_input=True,\n",
    "                        allow_inplace=True,\n",
    "                        use_cached_kv=not single_eval_pos,\n",
    "                        reuse_first_head_kv=True,\n",
    "                        use_second_set_of_queries=self.two_sets_of_queries,\n",
    "                    ).transpose(1, 2)\n",
    "                else:\n",
    "                    new_x_test = None\n",
    "\n",
    "                if single_eval_pos:\n",
    "                    new_x_train = self.self_attn_between_items(\n",
    "                        x[:, :single_eval_pos].transpose(1, 2),\n",
    "                        x[:, :single_eval_pos].transpose(1, 2),\n",
    "                        save_peak_mem_factor=save_peak_mem_factor,\n",
    "                        cache_kv=cache_trainset_representation,\n",
    "                        only_cache_first_head_kv=True,\n",
    "                        add_input=True,\n",
    "                        allow_inplace=True,\n",
    "                        use_cached_kv=False,\n",
    "                    ).transpose(1, 2)\n",
    "                else:\n",
    "                    new_x_train = None\n",
    "\n",
    "                return torch.cat(\n",
    "                    [x_ for x_ in [new_x_train, new_x_test] if x_ is not None],\n",
    "                    dim=1,\n",
    "                )\n",
    "\n",
    "            attention_src_x = None\n",
    "            if att_src is not None:\n",
    "                attention_src_x = att_src.transpose(1, 2)\n",
    "            elif single_eval_pos:\n",
    "                attention_src_x = x[:, :single_eval_pos].transpose(1, 2)\n",
    "\n",
    "            return self.self_attn_between_items(\n",
    "                x.transpose(1, 2),\n",
    "                attention_src_x,\n",
    "                save_peak_mem_factor=save_peak_mem_factor,\n",
    "                cache_kv=cache_trainset_representation and single_eval_pos,\n",
    "                add_input=True,\n",
    "                allow_inplace=True,\n",
    "                use_cached_kv=cache_trainset_representation and not single_eval_pos,\n",
    "            ).transpose(1, 2)\n",
    "\n",
    "        mlp_save_peak_mem_factor = (\n",
    "            save_peak_mem_factor * 8 if save_peak_mem_factor is not None else None\n",
    "        )\n",
    "\n",
    "        sublayers = []\n",
    "        if self.self_attn_between_features is not None:\n",
    "            sublayers.append(attn_between_features)\n",
    "        else:\n",
    "            assert state.shape[2] == 1, (\n",
    "                \"If there is no attention between features, the number of feature\"\n",
    "                \" blocks must be 1.\"\n",
    "            )\n",
    "\n",
    "        sublayers += [\n",
    "            attn_between_items,\n",
    "            partial(\n",
    "                self.mlp.__call__,\n",
    "                save_peak_mem_factor=mlp_save_peak_mem_factor\n",
    "                if (\n",
    "                    mlp_save_peak_mem_factor is not None\n",
    "                    and state.numel() // state.shape[-1] // mlp_save_peak_mem_factor\n",
    "                )\n",
    "                > 32\n",
    "                else None,\n",
    "                add_input=True,\n",
    "                allow_inplace=True,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        if self.second_mlp is not None:\n",
    "            sublayers.insert(\n",
    "                1,\n",
    "                partial(\n",
    "                    self.second_mlp.__call__,\n",
    "                    save_peak_mem_factor=mlp_save_peak_mem_factor,\n",
    "                    add_input=True,\n",
    "                    allow_inplace=True,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        for sublayer, layer_norm in zip(sublayers, self.layer_norms):\n",
    "            if self.pre_norm:\n",
    "                raise AssertionError(\n",
    "                    \"Pre-norm implementation is wrong, as the residual should never\"\n",
    "                    \" be layer normed here.\",\n",
    "                )\n",
    "                state = layer_norm(\n",
    "                    state,\n",
    "                    allow_inplace=True,\n",
    "                    save_peak_mem_factor=save_peak_mem_factor,\n",
    "                )\n",
    "\n",
    "            state = sublayer(state)\n",
    "            if not self.pre_norm:\n",
    "                state = layer_norm(\n",
    "                    state,\n",
    "                    allow_inplace=True,\n",
    "                    save_peak_mem_factor=save_peak_mem_factor,\n",
    "                )\n",
    "\n",
    "        return state\n",
    "\n",
    "    def empty_trainset_representation_cache(self) -> None:\n",
    "        \"\"\"Empty the trainset representation cache.\"\"\"\n",
    "        self.self_attn_between_items.empty_kv_cache()\n",
    "\n",
    "        # TODO: This could be None but this just ignored that fact here.\n",
    "        assert self.self_attn_between_features is not None\n",
    "        self.self_attn_between_features.empty_kv_cache()  # not necessary, just in case\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
